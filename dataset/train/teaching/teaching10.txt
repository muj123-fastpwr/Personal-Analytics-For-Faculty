766 Chapter 18. Learning from Examples 
18.14 This exercise concerns the expressiveness of decision lists (Section 18.5). 
a. Show that decision lists can represent any Boolean function, if the size of the tests is 
not limited. 
b. Show that if the tests can contain at most k literals each, then decision lists can represent 
any function that can be represented by a decision tree of depth k. 
18.15 Suppose a 7-nearest-neighbors  regression search  returns {7, 6, 8, 4, 7, 11,100} 
as the 
7 nearest y  values for a given x value. What is the value of y that minimizes the L loss 
i 
function on this data? There is a common name in statistics for this value as a function of the 
y values; what is it? Answer the same two questions for the L2 loss function. 
18.16 Figure 18.31 showed how a circle at the origin can be linearly separated by mapping 
from the features  ) to the two dimensions (x ,  TD.  Rut what if the circle is not located 
.
at the origin? What if it is an ellipse, not a circle? The general equation for a circle (and 
2 2 2
hence the decision boundary) is (x. —  0 + (x —  b) —  / =  0, and the general equation fat   
1 2 
2 2 
an ellipse is 
c(ri   — a) d(x2  — b) — 1=0. 
a. Expand out the equation for the circle and show what the weights w,   would be for the 
decision boundary in the four-dimensional feature space (x , T , xi, 4) Explain why 
1 2
this means that any circle is linearly separable in this space. 
five-dimensional feature space (xi, 
b. Do the same for ellipses in the 
x2,4,   x2, xi   r2)- 
18.17 Construct a support vector machine that computes the XOR function. Use values of 
+1 and —1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like 
11,  1) or ([-1, —11,  — 1). Map the input 'xi,   x2] into a space consisting of xi and xi x2. 
Draw the four input points in this space, and the maximal margin separator. What is the 
margin? Now draw the separating line back in the original Euclidean input space. 
18.18 Consider an ensemble learning algorithm that uses simple majority voting among 
K learned hypotheses. Suppose that each hypothesis has error c  and that the errors made 
by each hypothesis are independent of the others'. Calculate a formula for the error of the 
ensemble algorithm in terms of K and r, and evaluate it for the cases where K= 5, 10, and 
20 and e = 0.1. 0.2, and 0.4. If the independence assumption is removed, is it possible for the 
ensemble error to he worse than e9   
18.19 Construct by hand a neural network that computes the XOR function of two inputs. 
Make sure to specify what sort of units you are using. 
2
18.20 Recall from Chapter 18 that there are 2 ' distinct Boolean functions of n inputs. how 
many of these are representable by a threshold perceptron? 
18.21 Section 18.6.4 (page 725) noted that the output of the logistic function could be in- 
terpreted as a probability  p assigned by the model to the proposition that f (x) — 1; the prob- 
ability that f (x) = 0 is therefore 1 — p. Write down the probability p as a function of x 
and calculate the derivative of log p with respect to each weight wa. Repeat the process for 
log(1   —p). These calculations give a learning rule for minimizing the negative-log-likelihood   
